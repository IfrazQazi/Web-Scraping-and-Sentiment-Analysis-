{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web scraping and sentiment analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOVSjdCaA+vuf5FIq7L0bV+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IfrazQazi/Web-Scraping-and-Sentiment-Analysis-/blob/main/Web_scraping_and_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Objective**\n",
        "The objective of this assignment is to extract textual data articles from the given URL and perform text analysis to compute variables that are explained below. \n",
        "\n",
        "##**Data Extraction**\n",
        "\n",
        "\n",
        "For each of the articles, given in the input.xlsx file, extract the article text and save the extracted article in a text file with URL_ID as its file name.\n",
        "While extracting text, please make sure your program extracts only the article title and the article text. It should not extract the website header, footer, or anything other than the article text. \n",
        "\n",
        "##**Data Analysis**\n",
        "For each of the extracted texts from the article, perform textual analysis and compute variables.\n",
        "##**Variables**\n",
        "\n",
        "* POSITIVE SCORE\n",
        "* NEGATIVE SCORE\n",
        "* POLARITY SCORE\n",
        "* SUBJECTIVITY SCORE\n",
        "* AVG SENTENCE LENGTH\n",
        "* PERCENTAGE OF COMPLEX WORDS\n",
        "* FOG INDEX\n",
        "* AVG NUMBER OF WORDS PER SENTENCE\n",
        "* COMPLEX WORD COUNT\n",
        "* WORD COUNT\n",
        "* SYLLABLE PER WORD\n",
        "* PERSONAL PRONOUNS\n",
        "* AVG WORD LENGTH\n",
        "\n"
      ],
      "metadata": {
        "id": "LSpjbEwXmW1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets understand variables\n"
      ],
      "metadata": {
        "id": "MVduBcQbo-QC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Sentimental Analysis**\n",
        "## Sentimental analysis is the process of determining whether a piece of writing is positive, negative or neutral.\n",
        "* **POSITIVE SCORE**\n",
        "* **NEGATIVE SCORE**\n",
        "\n",
        "##**POLARITY SCORE**\n",
        "## Polarity lies between [-1,1], -1 defines a negative sentiment and 1 defines a positive sentiment.\n",
        "\n",
        "##**SUBJECTIVITY SCORE**\n",
        "## Subjectivity quantifies the amount of personal opinion and factual information contained in the text.\n",
        "## The higher subjectivity means that the text contains personal opinion rather than factual information."
      ],
      "metadata": {
        "id": "fvnw46HJprL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Analysis of Readability**\n",
        "##Analysis of Readability is calculated using the Gunning Fox index formula described below.\n",
        "* **Average Sentence Length** = the number of words / the number of sentences\n",
        "\n",
        "* **Percentage of Complex words** = the number of complex words / the number of words\n",
        "\n",
        "* **Fog Index** = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
        "\n",
        "*  **Fog Index :**\n",
        "\n",
        "  The fog index is commonly used to confirm that text can be read easily by the intended audience. Texts for a wide audience generally need a fog index less than 12. Texts requiring near-universal understanding generally need an index less than 8."
      ],
      "metadata": {
        "id": "2GnZ71L_toU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Average Number of Words Per Sentence**\n",
        "##The formula for calculating is:\n",
        "**Average Number of Words Per Sentence** = the total number of words / the total number of sentences\n",
        "\n",
        "## **Complex Word Count**\n",
        " ## Complex words are words in the text that contain more than two syllables.\n",
        "\n",
        "## **Word Count**\n",
        "## We count the total cleaned words present in the text by \n",
        "1. removing the stop words (using stopwords class of nltk package).\n",
        "2. removing any punctuations like ? ! , . from the word before counting.\n",
        "\n"
      ],
      "metadata": {
        "id": "uxIdjEqLvDfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Syllable Count Per Word :**\n",
        "## We count the number of Syllables in each word of the text by counting the vowels present in each word. We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n",
        "\n",
        "## **Personal Pronouns**\n",
        "## To calculate Personal Pronouns mentioned in the text, we use regex to find the counts of the words\n",
        "\n",
        "* “I,” “we,” “my,” “ours,” and “us”. Special care is taken so that the country name US is not included in the list.\n",
        "\n",
        "##**Average Word Length:**\n",
        "## Average Word Length is calculated by the formula:\n",
        "* Sum of the total number of characters in each word/Total number of words\n"
      ],
      "metadata": {
        "id": "NoKxczKUwQde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So first lets import libraries"
      ],
      "metadata": {
        "id": "E99Khv9QxtQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## importing libraries\n",
        "import pandas as pd\n",
        "import requests\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup "
      ],
      "metadata": {
        "id": "lmcGCBcspK-V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk                 ## importing nltk library\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "wLAV0aRAx1sr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee98ed1-7629-4c8c-bdad-75825f70a038"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "NLOQjGfH9WBv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating some function which we will use later"
      ],
      "metadata": {
        "id": "4H0lYUKS_ZWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the stopwords from nltk library\n",
        "sw = stopwords.words('english')"
      ],
      "metadata": {
        "id": "W_19jiY_9cNz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords(text):\n",
        "    '''a function for removing the stopword'''\n",
        "    # removing the stop words and lowercasing the selected words\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "QeBvROgU9fSH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def number_of_sentences(text): ## number of sentences in paragraph\n",
        "    number_of_sentence = sent_tokenize(text)\n",
        "    count_of_sentences = len(number_of_sentence)\n",
        "    return count_of_sentences\n",
        "# print(number_of_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plSfYHYx9q2z",
        "outputId": "b3f2e4c5-5bd4-40fc-c629-0c847115889c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    import string\n",
        "    # replacing the punctuations with no space, \n",
        "    # which in effect deletes the punctuation marks \n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # return the text stripped of punctuation marks\n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "gqwywq8G9xlP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CuYKs50N96bX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}